{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4df316",
   "metadata": {},
   "source": [
    "# **Training an Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49ec001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8e7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_files():\n",
    "\n",
    "    base_dir = os.path.abspath(os.path.dirname(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "    data_dir = os.path.join(base_dir, 'Processed Data')\n",
    "\n",
    "    X_train_path = os.path.join(data_dir, 'X_train_processed.npy')\n",
    "    y_train_path = os.path.join(data_dir, 'y_train_processed.npy')\n",
    "    X_test_path = os.path.join(data_dir, 'X_test_processed.npy')\n",
    "    y_test_path = os.path.join(data_dir, 'y_test_processed.npy')\n",
    "\n",
    "    X_train_mx = np.load(X_train_path, allow_pickle = True).item()\n",
    "    y_train_mx = np.load(y_train_path, allow_pickle = True)\n",
    "    X_test_mx = np.load(X_test_path, allow_pickle = True).item()\n",
    "    y_test_mx = np.load(y_test_path, allow_pickle = True)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train_mx.toarray())\n",
    "    y_train = pd.Series(y_train_mx.ravel())\n",
    "    X_test = pd.DataFrame(X_test_mx.toarray())\n",
    "    y_test = pd.Series(y_test_mx.ravel())\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69760ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fea73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_column_names():\n",
    "\n",
    "    base_dir = os.path.abspath(os.path.dirname(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "    data_dir = os.path.join(base_dir, 'Processing Objects')\n",
    "\n",
    "    column_names_path = os.path.join(data_dir, 'the_preprocessor.joblib')\n",
    "\n",
    "    column_names_pre = joblib.load(column_names_path)\n",
    "\n",
    "    column_names = column_names_pre.get_feature_names_out()\n",
    "\n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5647778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "\n",
    "column_names = load_column_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2af5bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the name with the corresponding column\n",
    "\n",
    "X_train.columns = column_names\n",
    "X_test.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7debca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numerical__Age', 'numerical__EducationLevel', 'numerical__BMI', 'numerical__CognitiveTestScore', 'binary_categorical__Gender_Male', 'binary_categorical__Diabetes_Yes', 'binary_categorical__Hypertension_Yes', 'binary_categorical__CholesterolLevel_Normal', 'binary_categorical__AlzheimerInFamily_Yes', 'binary_categorical__GeneticRisk_Yes', 'binary_categorical__UrbanRural_Urban', 'multi_binary_categorical__Country_Argentina', 'multi_binary_categorical__Country_Australia', 'multi_binary_categorical__Country_Brazil', 'multi_binary_categorical__Country_Canada', 'multi_binary_categorical__Country_China', 'multi_binary_categorical__Country_France', 'multi_binary_categorical__Country_Germany', 'multi_binary_categorical__Country_India', 'multi_binary_categorical__Country_Italy', 'multi_binary_categorical__Country_Japan', 'multi_binary_categorical__Country_Mexico', 'multi_binary_categorical__Country_Norway', 'multi_binary_categorical__Country_Russia', 'multi_binary_categorical__Country_Saudi Arabia', 'multi_binary_categorical__Country_South Africa', 'multi_binary_categorical__Country_South Korea', 'multi_binary_categorical__Country_Spain', 'multi_binary_categorical__Country_Sweden', 'multi_binary_categorical__Country_UK', 'multi_binary_categorical__Country_USA', 'multi_binary_categorical__PhysicalActivityLevel_High', 'multi_binary_categorical__PhysicalActivityLevel_Low', 'multi_binary_categorical__PhysicalActivityLevel_Medium', 'multi_binary_categorical__SmokingStatus_Current', 'multi_binary_categorical__SmokingStatus_Former', 'multi_binary_categorical__SmokingStatus_Never', 'multi_binary_categorical__AlcoholConsumption_Never', 'multi_binary_categorical__AlcoholConsumption_Occasionally', 'multi_binary_categorical__AlcoholConsumption_Regularly', 'multi_binary_categorical__DepressionLevel_High', 'multi_binary_categorical__DepressionLevel_Low', 'multi_binary_categorical__DepressionLevel_Medium', 'multi_binary_categorical__SleepQuality_Average', 'multi_binary_categorical__SleepQuality_Good', 'multi_binary_categorical__SleepQuality_Poor', 'multi_binary_categorical__DietaryHabits_Average', 'multi_binary_categorical__DietaryHabits_Healthy', 'multi_binary_categorical__DietaryHabits_Unhealthy', 'multi_binary_categorical__AirPollutionExposure_High', 'multi_binary_categorical__AirPollutionExposure_Low', 'multi_binary_categorical__AirPollutionExposure_Medium', 'multi_binary_categorical__EmploymentStatus_Employed', 'multi_binary_categorical__EmploymentStatus_Retired', 'multi_binary_categorical__EmploymentStatus_Unemployed', 'multi_binary_categorical__MaritalStatus_Married', 'multi_binary_categorical__MaritalStatus_Single', 'multi_binary_categorical__MaritalStatus_Widowed', 'multi_binary_categorical__SocialEngagementLevel_High', 'multi_binary_categorical__SocialEngagementLevel_Low', 'multi_binary_categorical__SocialEngagementLevel_Medium', 'multi_binary_categorical__IncomeLevel_High', 'multi_binary_categorical__IncomeLevel_Low', 'multi_binary_categorical__IncomeLevel_Medium', 'multi_binary_categorical__StressLevels_High', 'multi_binary_categorical__StressLevels_Low', 'multi_binary_categorical__StressLevels_Medium']\n"
     ]
    }
   ],
   "source": [
    "print(column_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2f51b",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1fe2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a70c076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Cross Validation\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326be7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save the results\n",
    "\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88d578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with Optuna (Objective Functions)\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "# Personalized Wrapping, to ensure that XGBoost Classifier behaves like a Sklearn method\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class SklearnXGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.model = XGBClassifier(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def get_params(self, deep = True):\n",
    "        \n",
    "        return self.model.get_params(deep)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \n",
    "        self.model.set_params(**params)\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    \n",
    "    model = SklearnXGBClassifier(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150),\n",
    "        max_depth = trial.suggest_int('max_depth', 5, 15),\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-2, 1e-1, log = True),\n",
    "        subsample = trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        eval_metric = 'logloss',random_state = 42,n_jobs = -1\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "\n",
    "# Stacking\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def stacking_objective(trial):\n",
    "\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators = 50, random_state = 42)),\n",
    "        ('dt', DecisionTreeClassifier(min_samples_leaf = 2, random_state = 42)),\n",
    "        ('gbc', GradientBoostingClassifier(n_estimators = 50, random_state = 42))\n",
    "    ]\n",
    "\n",
    "    meta_estimator = LogisticRegression(\n",
    "        C = trial.suggest_float('C', 1e-2, 10.0, log = True),\n",
    "        max_iter = 1000,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    model = StackingClassifier(\n",
    "        estimators = estimators,\n",
    "        final_estimator = meta_estimator,\n",
    "        passthrough = trial.suggest_categorical('passthrough', [True, False]),\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_objective(trial):\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 100),\n",
    "        max_depth = trial.suggest_int('max_depth', 15, 50),\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10),\n",
    "        random_state = 42, n_jobs = -1\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "# LightGBM\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def lgbm_objective(trial):\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150),\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 15),\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log = True),\n",
    "        num_leaves = trial.suggest_int('num_leaves', 20, 100),\n",
    "        subsample = trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        random_state = 42, n_jobs=-1, verbosity = -1\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "\n",
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_objective(trial):\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors = trial.suggest_int('n_neighbors', 3, 8),\n",
    "        metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def dt_objective(trial):\n",
    "\n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 12),\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 7),\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 7),\n",
    "        criterion = trial.suggest_categorical('criterion', ['gini', 'entropy']), \n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "# Multi-Layer Perceptron\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp_objective(trial):\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes = (trial.suggest_int('hl1', 10, 40),\n",
    "                              trial.suggest_int('hl2', 10, 50)),\n",
    "        activation = trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        alpha = trial.suggest_float('alpha', 1e-3, 1e-1, log = True),\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
    "        learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log = True),\n",
    "        max_iter = 500, random_state = 42\n",
    "        \n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "# Gradient Boost Classifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def gbc_objective(trial):\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 150),\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log = True),\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10),\n",
    "        subsample = trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
    "\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_objective(trial):\n",
    "\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log = True)\n",
    "    model = SVC(kernel = kernel, gamma = gamma)\n",
    "\n",
    "    return cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f84ecada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:46:13,148] A new study created in memory with name: no-name-a2746b18-ed60-4aa5-a0a7-8197eba6c2b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizating LightGBM  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:46:16,415] Trial 0 finished with value: 0.7072194663688076 and parameters: {'n_estimators': 116, 'max_depth': 13, 'learning_rate': 0.010643877203710038, 'num_leaves': 72, 'subsample': 0.7066337035847874, 'colsample_bytree': 0.7031257684047441}. Best is trial 0 with value: 0.7072194663688076.\n",
      "[I 2025-06-17 11:46:18,079] Trial 1 finished with value: 0.7204892371307892 and parameters: {'n_estimators': 131, 'max_depth': 5, 'learning_rate': 0.030128576176060066, 'num_leaves': 58, 'subsample': 0.949786071242218, 'colsample_bytree': 0.8922775142023589}. Best is trial 1 with value: 0.7204892371307892.\n",
      "[I 2025-06-17 11:46:18,773] Trial 2 finished with value: 0.6076387781369511 and parameters: {'n_estimators': 80, 'max_depth': 3, 'learning_rate': 0.0031133302428775636, 'num_leaves': 26, 'subsample': 0.6532210406120806, 'colsample_bytree': 0.9768889730661925}. Best is trial 1 with value: 0.7204892371307892.\n",
      "[I 2025-06-17 11:46:19,608] Trial 3 finished with value: 0.7217969812186084 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.09867066541086178, 'num_leaves': 44, 'subsample': 0.5329032825641946, 'colsample_bytree': 0.5109418476113343}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:20,745] Trial 4 finished with value: 0.6563521252043466 and parameters: {'n_estimators': 108, 'max_depth': 6, 'learning_rate': 0.00484990021276931, 'num_leaves': 85, 'subsample': 0.8116005488836262, 'colsample_bytree': 0.6349290664574241}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:21,629] Trial 5 finished with value: 0.7205084642014394 and parameters: {'n_estimators': 64, 'max_depth': 13, 'learning_rate': 0.03425289704695774, 'num_leaves': 42, 'subsample': 0.9530006124539399, 'colsample_bytree': 0.8910672719977113}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:22,462] Trial 6 finished with value: 0.7005653539171665 and parameters: {'n_estimators': 76, 'max_depth': 8, 'learning_rate': 0.007866544607963388, 'num_leaves': 26, 'subsample': 0.9894950344067956, 'colsample_bytree': 0.8855650447303116}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:23,727] Trial 7 finished with value: 0.7055079075650765 and parameters: {'n_estimators': 106, 'max_depth': 12, 'learning_rate': 0.010356448179674188, 'num_leaves': 52, 'subsample': 0.877038156546651, 'colsample_bytree': 0.6374484041331134}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:24,762] Trial 8 finished with value: 0.7143351709114041 and parameters: {'n_estimators': 57, 'max_depth': 10, 'learning_rate': 0.03773880812307753, 'num_leaves': 95, 'subsample': 0.6862848135940729, 'colsample_bytree': 0.543326293662745}. Best is trial 3 with value: 0.7217969812186084.\n",
      "[I 2025-06-17 11:46:26,448] Trial 9 finished with value: 0.7057002097095135 and parameters: {'n_estimators': 103, 'max_depth': 13, 'learning_rate': 0.00822821627562163, 'num_leaves': 87, 'subsample': 0.9460714195497055, 'colsample_bytree': 0.8916505480564558}. Best is trial 3 with value: 0.7217969812186084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7217969812186084\n",
      "Hyperparameters:  {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.09867066541086178, 'num_leaves': 44, 'subsample': 0.5329032825641946, 'colsample_bytree': 0.5109418476113343}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:46:26,658] A new study created in memory with name: no-name-27a01f9e-aa43-4447-aeb5-eb2971dd913b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7221449405429661\n",
      "Optimizating XGBoost  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:46:31,302] Trial 0 finished with value: 0.7043732625918173 and parameters: {'n_estimators': 56, 'max_depth': 15, 'learning_rate': 0.017071387077126386, 'subsample': 0.6333763560424628}. Best is trial 0 with value: 0.7043732625918173.\n",
      "[I 2025-06-17 11:46:32,837] Trial 1 finished with value: 0.7195853817304918 and parameters: {'n_estimators': 133, 'max_depth': 5, 'learning_rate': 0.011249505482921878, 'subsample': 0.5457303155167652}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:39,068] Trial 2 finished with value: 0.709719667941444 and parameters: {'n_estimators': 84, 'max_depth': 14, 'learning_rate': 0.01672942833529301, 'subsample': 0.7335677476583053}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:40,012] Trial 3 finished with value: 0.703680919762995 and parameters: {'n_estimators': 69, 'max_depth': 5, 'learning_rate': 0.01226830470283383, 'subsample': 0.7717140089301568}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:46,971] Trial 4 finished with value: 0.7070080000295886 and parameters: {'n_estimators': 127, 'max_depth': 13, 'learning_rate': 0.04021327164913601, 'subsample': 0.9766115269183261}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:48,601] Trial 5 finished with value: 0.7159121032347785 and parameters: {'n_estimators': 89, 'max_depth': 8, 'learning_rate': 0.053711542148464984, 'subsample': 0.7241961847996282}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:50,404] Trial 6 finished with value: 0.7138543887356033 and parameters: {'n_estimators': 77, 'max_depth': 9, 'learning_rate': 0.015711297755386832, 'subsample': 0.6008957205479133}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:55,426] Trial 7 finished with value: 0.7115658772663052 and parameters: {'n_estimators': 79, 'max_depth': 14, 'learning_rate': 0.0276872547172734, 'subsample': 0.537221368461912}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:46:59,372] Trial 8 finished with value: 0.695853647170216 and parameters: {'n_estimators': 53, 'max_depth': 15, 'learning_rate': 0.013201200392370058, 'subsample': 0.5591310679230624}. Best is trial 1 with value: 0.7195853817304918.\n",
      "[I 2025-06-17 11:47:04,124] Trial 9 finished with value: 0.7078733384127172 and parameters: {'n_estimators': 110, 'max_depth': 12, 'learning_rate': 0.05861183225614008, 'subsample': 0.9374824565511358}. Best is trial 1 with value: 0.7195853817304918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7195853817304918\n",
      "Hyperparameters:  {'n_estimators': 133, 'max_depth': 5, 'learning_rate': 0.011249505482921878, 'subsample': 0.5457303155167652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:47:04,513] A new study created in memory with name: no-name-ab8be9c3-a453-4769-b587-7d9fa9ab97ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7207987435494727\n",
      "Optimizating Stacking  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 11:48:30,420] Trial 0 finished with value: 0.7206430832846353 and parameters: {'C': 4.193485461385948, 'passthrough': False}. Best is trial 0 with value: 0.7206430832846353.\n",
      "[I 2025-06-17 11:49:49,036] Trial 1 finished with value: 0.7191430555452817 and parameters: {'C': 0.05071084084341889, 'passthrough': True}. Best is trial 0 with value: 0.7206430832846353.\n",
      "[I 2025-06-17 11:51:03,088] Trial 2 finished with value: 0.7204892648701428 and parameters: {'C': 0.04845238950484598, 'passthrough': False}. Best is trial 0 with value: 0.7206430832846353.\n",
      "[I 2025-06-17 11:52:16,954] Trial 3 finished with value: 0.7207007885373594 and parameters: {'C': 0.1308652951770435, 'passthrough': False}. Best is trial 3 with value: 0.7207007885373594.\n",
      "[I 2025-06-17 11:53:35,538] Trial 4 finished with value: 0.7206046772248811 and parameters: {'C': 0.11190896666998035, 'passthrough': True}. Best is trial 3 with value: 0.7207007885373594.\n",
      "[I 2025-06-17 11:54:55,539] Trial 5 finished with value: 0.7208546457869469 and parameters: {'C': 0.6755970383033681, 'passthrough': False}. Best is trial 5 with value: 0.7208546457869469.\n",
      "[I 2025-06-17 11:56:14,306] Trial 6 finished with value: 0.7206238506661145 and parameters: {'C': 8.903400323326624, 'passthrough': False}. Best is trial 5 with value: 0.7208546457869469.\n",
      "[I 2025-06-17 11:57:33,000] Trial 7 finished with value: 0.7207584900915028 and parameters: {'C': 0.4370412156452656, 'passthrough': False}. Best is trial 5 with value: 0.7208546457869469.\n",
      "[I 2025-06-17 11:58:54,509] Trial 8 finished with value: 0.7179891705563405 and parameters: {'C': 0.029579233774269964, 'passthrough': True}. Best is trial 5 with value: 0.7208546457869469.\n",
      "[I 2025-06-17 12:00:13,565] Trial 9 finished with value: 0.719604638389786 and parameters: {'C': 0.02470295541960244, 'passthrough': False}. Best is trial 5 with value: 0.7208546457869469.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7208546457869469\n",
      "Hyperparameters:  {'C': 0.6755970383033681, 'passthrough': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:00:34,974] A new study created in memory with name: no-name-5d5b9664-5671-492c-8af1-232fc928d68c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7196769127215615\n",
      "Optimizating Random Forest  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:00:39,326] Trial 0 finished with value: 0.7117966538942354 and parameters: {'n_estimators': 75, 'max_depth': 39, 'min_samples_leaf': 1, 'min_samples_split': 5}. Best is trial 0 with value: 0.7117966538942354.\n",
      "[I 2025-06-17 12:00:42,604] Trial 1 finished with value: 0.7157391113790528 and parameters: {'n_estimators': 63, 'max_depth': 18, 'min_samples_leaf': 4, 'min_samples_split': 6}. Best is trial 1 with value: 0.7157391113790528.\n",
      "[I 2025-06-17 12:00:45,670] Trial 2 finished with value: 0.7117967204686841 and parameters: {'n_estimators': 52, 'max_depth': 26, 'min_samples_leaf': 1, 'min_samples_split': 5}. Best is trial 1 with value: 0.7157391113790528.\n",
      "[I 2025-06-17 12:00:50,296] Trial 3 finished with value: 0.7177391687070502 and parameters: {'n_estimators': 97, 'max_depth': 24, 'min_samples_leaf': 8, 'min_samples_split': 8}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:00:53,142] Trial 4 finished with value: 0.7153351968014675 and parameters: {'n_estimators': 56, 'max_depth': 31, 'min_samples_leaf': 9, 'min_samples_split': 9}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:00:57,179] Trial 5 finished with value: 0.7174506775799447 and parameters: {'n_estimators': 81, 'max_depth': 50, 'min_samples_leaf': 7, 'min_samples_split': 2}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:01:00,980] Trial 6 finished with value: 0.711758109137713 and parameters: {'n_estimators': 63, 'max_depth': 24, 'min_samples_leaf': 1, 'min_samples_split': 3}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:01:04,300] Trial 7 finished with value: 0.7163929223963843 and parameters: {'n_estimators': 68, 'max_depth': 22, 'min_samples_leaf': 9, 'min_samples_split': 6}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:01:07,713] Trial 8 finished with value: 0.714392859520516 and parameters: {'n_estimators': 61, 'max_depth': 38, 'min_samples_leaf': 1, 'min_samples_split': 10}. Best is trial 3 with value: 0.7177391687070502.\n",
      "[I 2025-06-17 12:01:13,264] Trial 9 finished with value: 0.7140467814952621 and parameters: {'n_estimators': 94, 'max_depth': 48, 'min_samples_leaf': 1, 'min_samples_split': 3}. Best is trial 3 with value: 0.7177391687070502.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7177391687070502\n",
      "Hyperparameters:  {'n_estimators': 97, 'max_depth': 24, 'min_samples_leaf': 8, 'min_samples_split': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:01:18,642] A new study created in memory with name: no-name-d6a80983-178e-4879-be42-eea8b4cada23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7185550818936505\n",
      "Optimizating KNN  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:01:22,920] Trial 0 finished with value: 0.6394092386841931 and parameters: {'n_neighbors': 6, 'metric': 'minkowski'}. Best is trial 0 with value: 0.6394092386841931.\n",
      "[I 2025-06-17 12:01:27,084] Trial 1 finished with value: 0.6423901466117303 and parameters: {'n_neighbors': 5, 'metric': 'minkowski'}. Best is trial 1 with value: 0.6423901466117303.\n",
      "[I 2025-06-17 12:01:46,533] Trial 2 finished with value: 0.619773841049805 and parameters: {'n_neighbors': 4, 'metric': 'manhattan'}. Best is trial 1 with value: 0.6423901466117303.\n",
      "[I 2025-06-17 12:01:50,782] Trial 3 finished with value: 0.6481980312456079 and parameters: {'n_neighbors': 8, 'metric': 'euclidean'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:10,475] Trial 4 finished with value: 0.6347166702419612 and parameters: {'n_neighbors': 8, 'metric': 'manhattan'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:30,143] Trial 5 finished with value: 0.626158719033635 and parameters: {'n_neighbors': 5, 'metric': 'manhattan'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:34,333] Trial 6 finished with value: 0.6423901466117303 and parameters: {'n_neighbors': 5, 'metric': 'euclidean'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:38,517] Trial 7 finished with value: 0.6481980312456079 and parameters: {'n_neighbors': 8, 'metric': 'euclidean'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:42,722] Trial 8 finished with value: 0.6423901466117303 and parameters: {'n_neighbors': 5, 'metric': 'euclidean'}. Best is trial 3 with value: 0.6481980312456079.\n",
      "[I 2025-06-17 12:02:46,841] Trial 9 finished with value: 0.6320820234194116 and parameters: {'n_neighbors': 4, 'metric': 'minkowski'}. Best is trial 3 with value: 0.6481980312456079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.6481980312456079\n",
      "Hyperparameters:  {'n_neighbors': 8, 'metric': 'euclidean'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:02:48,918] A new study created in memory with name: no-name-15f58fd0-7df4-4b7b-b166-0f1b93bb6539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.6537581332735024\n",
      "Optimizating Decision Tree  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:02:49,804] Trial 0 finished with value: 0.7188930056144451 and parameters: {'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 6, 'criterion': 'entropy'}. Best is trial 0 with value: 0.7188930056144451.\n",
      "[I 2025-06-17 12:02:50,893] Trial 1 finished with value: 0.7139312748267215 and parameters: {'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 6, 'criterion': 'entropy'}. Best is trial 0 with value: 0.7188930056144451.\n",
      "[I 2025-06-17 12:02:52,095] Trial 2 finished with value: 0.7130658680198538 and parameters: {'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 0 with value: 0.7188930056144451.\n",
      "[I 2025-06-17 12:02:52,734] Trial 3 finished with value: 0.7203161602077122 and parameters: {'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:53,588] Trial 4 finished with value: 0.718719912047756 and parameters: {'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 3, 'criterion': 'gini'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:54,913] Trial 5 finished with value: 0.7062771494300488 and parameters: {'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:56,327] Trial 6 finished with value: 0.7036423879515044 and parameters: {'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 2, 'criterion': 'entropy'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:57,630] Trial 7 finished with value: 0.7092771697722414 and parameters: {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:59,037] Trial 8 finished with value: 0.7035654648745812 and parameters: {'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 3 with value: 0.7203161602077122.\n",
      "[I 2025-06-17 12:02:59,557] Trial 9 finished with value: 0.7204315411245165 and parameters: {'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 9 with value: 0.7204315411245165.\n",
      "[I 2025-06-17 12:02:59,690] A new study created in memory with name: no-name-f1cabd80-5add-43d9-8de7-57165fe5d7f1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7204315411245165\n",
      "Hyperparameters:  {'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4, 'criterion': 'gini'}\n",
      "Accuracy in test:  0.7199012788871438\n",
      "Optimizating MLP  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-06-17 12:07:47,712] Trial 0 finished with value: 0.6639292757439694 and parameters: {'hl1': 36, 'hl2': 50, 'activation': 'tanh', 'alpha': 0.009800478381999171, 'learning_rate': 'adaptive', 'learning_rate_init': 0.000905307402605792}. Best is trial 0 with value: 0.6639292757439694.\n",
      "[I 2025-06-17 12:08:13,800] Trial 1 finished with value: 0.7046423879515042 and parameters: {'hl1': 21, 'hl2': 43, 'activation': 'tanh', 'alpha': 0.003091993464675863, 'learning_rate': 'adaptive', 'learning_rate_init': 0.022099377984116313}. Best is trial 1 with value: 0.7046423879515042.\n",
      "[I 2025-06-17 12:09:59,222] Trial 2 finished with value: 0.6901612766020401 and parameters: {'hl1': 25, 'hl2': 25, 'activation': 'tanh', 'alpha': 0.021025637167786368, 'learning_rate': 'adaptive', 'learning_rate_init': 0.0014897291261845044}. Best is trial 1 with value: 0.7046423879515042.\n",
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:780: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-06-17 12:14:16,436] Trial 3 finished with value: 0.689815067277179 and parameters: {'hl1': 34, 'hl2': 25, 'activation': 'tanh', 'alpha': 0.022168180874429733, 'learning_rate': 'constant', 'learning_rate_init': 0.00032987539426577203}. Best is trial 1 with value: 0.7046423879515042.\n",
      "[I 2025-06-17 12:14:27,721] Trial 4 finished with value: 0.7118544571593423 and parameters: {'hl1': 24, 'hl2': 26, 'activation': 'relu', 'alpha': 0.0328281113871932, 'learning_rate': 'constant', 'learning_rate_init': 0.05063955044048914}. Best is trial 4 with value: 0.7118544571593423.\n",
      "[I 2025-06-17 12:15:25,388] Trial 5 finished with value: 0.6853532754628773 and parameters: {'hl1': 34, 'hl2': 22, 'activation': 'relu', 'alpha': 0.0024420543420405187, 'learning_rate': 'constant', 'learning_rate_init': 0.005560146750683907}. Best is trial 4 with value: 0.7118544571593423.\n",
      "[I 2025-06-17 12:17:16,651] Trial 6 finished with value: 0.6934112433148159 and parameters: {'hl1': 13, 'hl2': 40, 'activation': 'tanh', 'alpha': 0.005568172038376417, 'learning_rate': 'adaptive', 'learning_rate_init': 0.0016146801610855482}. Best is trial 4 with value: 0.7118544571593423.\n",
      "[I 2025-06-17 12:19:30,174] Trial 7 finished with value: 0.6755452095985561 and parameters: {'hl1': 39, 'hl2': 36, 'activation': 'relu', 'alpha': 0.0031361099679042205, 'learning_rate': 'adaptive', 'learning_rate_init': 0.0017327347099047096}. Best is trial 4 with value: 0.7118544571593423.\n",
      "[I 2025-06-17 12:22:41,897] Trial 8 finished with value: 0.7009500710127453 and parameters: {'hl1': 35, 'hl2': 43, 'activation': 'relu', 'alpha': 0.06420738422547558, 'learning_rate': 'adaptive', 'learning_rate_init': 0.0001425823066695409}. Best is trial 4 with value: 0.7118544571593423.\n",
      "[I 2025-06-17 12:23:42,120] Trial 9 finished with value: 0.6977192130160444 and parameters: {'hl1': 18, 'hl2': 23, 'activation': 'relu', 'alpha': 0.008114976515484183, 'learning_rate': 'adaptive', 'learning_rate_init': 0.0015100486370051192}. Best is trial 4 with value: 0.7118544571593423.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7118544571593423\n",
      "Hyperparameters:  {'hl1': 24, 'hl2': 26, 'activation': 'relu', 'alpha': 0.0328281113871932, 'learning_rate': 'constant', 'learning_rate_init': 0.05063955044048914}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:23:44,689] A new study created in memory with name: no-name-d30a9160-d65d-43b2-88eb-97a1241a7729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7152344626430335\n",
      "Optimizating Gradient Boosting  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:24:14,100] Trial 0 finished with value: 0.5865417902608978 and parameters: {'n_estimators': 55, 'learning_rate': 0.0012249505348296075, 'max_depth': 6, 'subsample': 0.7108322765015573}. Best is trial 0 with value: 0.5865417902608978.\n",
      "[I 2025-06-17 12:24:46,028] Trial 1 finished with value: 0.5865417902608978 and parameters: {'n_estimators': 59, 'learning_rate': 0.001384789817698933, 'max_depth': 6, 'subsample': 0.7402586879915636}. Best is trial 0 with value: 0.5865417902608978.\n",
      "[I 2025-06-17 12:25:32,540] Trial 2 finished with value: 0.7203161472626807 and parameters: {'n_estimators': 133, 'learning_rate': 0.017722416290695236, 'max_depth': 3, 'subsample': 0.8710330565191053}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:26:14,212] Trial 3 finished with value: 0.5865417902608978 and parameters: {'n_estimators': 50, 'learning_rate': 0.001383421713514515, 'max_depth': 8, 'subsample': 0.8662631339729905}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:27:48,042] Trial 4 finished with value: 0.6494288134214088 and parameters: {'n_estimators': 84, 'learning_rate': 0.0036822493976903675, 'max_depth': 10, 'subsample': 0.8502585583935571}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:30:54,571] Trial 5 finished with value: 0.6984115059140302 and parameters: {'n_estimators': 148, 'learning_rate': 0.005349464929959275, 'max_depth': 10, 'subsample': 0.9788587663673458}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:31:40,412] Trial 6 finished with value: 0.6883533697766797 and parameters: {'n_estimators': 122, 'learning_rate': 0.003753406123480725, 'max_depth': 4, 'subsample': 0.749676527144084}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:33:05,477] Trial 7 finished with value: 0.7184506775799449 and parameters: {'n_estimators': 105, 'learning_rate': 0.021306253232755333, 'max_depth': 7, 'subsample': 0.9714086918318035}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:33:43,250] Trial 8 finished with value: 0.6967767666269686 and parameters: {'n_estimators': 51, 'learning_rate': 0.011364438538696495, 'max_depth': 7, 'subsample': 0.8817206390922137}. Best is trial 2 with value: 0.7203161472626807.\n",
      "[I 2025-06-17 12:34:02,161] Trial 9 finished with value: 0.6839108586624454 and parameters: {'n_estimators': 64, 'learning_rate': 0.008025564968196713, 'max_depth': 3, 'subsample': 0.6994763548745726}. Best is trial 2 with value: 0.7203161472626807.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7203161472626807\n",
      "Hyperparameters:  {'n_estimators': 133, 'learning_rate': 0.017722416290695236, 'max_depth': 3, 'subsample': 0.8710330565191053}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 12:34:14,844] A new study created in memory with name: no-name-0a592a49-b16c-4c31-8f3e-fa1497a19559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test:  0.7199461521202603\n",
      "Optimizating SVM  ...\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 13:03:38,261] Trial 0 finished with value: 0.7126428095896794 and parameters: {'kernel': 'linear', 'gamma': 0.0013352444014453847}. Best is trial 0 with value: 0.7126428095896794.\n",
      "[I 2025-06-17 13:33:50,011] Trial 1 finished with value: 0.7126428095896794 and parameters: {'kernel': 'linear', 'gamma': 0.0928752276724382}. Best is trial 0 with value: 0.7126428095896794.\n",
      "[I 2025-06-17 13:45:18,129] Trial 2 finished with value: 0.7092003890906671 and parameters: {'kernel': 'rbf', 'gamma': 0.01799727470933265}. Best is trial 0 with value: 0.7126428095896794.\n",
      "[I 2025-06-17 13:56:46,519] Trial 3 finished with value: 0.713969804788922 and parameters: {'kernel': 'rbf', 'gamma': 0.0020622454773098968}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 14:08:17,589] Trial 4 finished with value: 0.7087003983371183 and parameters: {'kernel': 'rbf', 'gamma': 0.0209961106581337}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 14:19:39,960] Trial 5 finished with value: 0.7137390429553137 and parameters: {'kernel': 'rbf', 'gamma': 0.002485888226131653}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 14:32:05,172] Trial 6 finished with value: 0.7114119682365908 and parameters: {'kernel': 'sigmoid', 'gamma': 0.0001684506539477806}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 14:42:37,034] Trial 7 finished with value: 0.7136428872598697 and parameters: {'kernel': 'sigmoid', 'gamma': 0.006002214955811101}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 15:12:36,515] Trial 8 finished with value: 0.7126428095896794 and parameters: {'kernel': 'linear', 'gamma': 0.0006011920783627712}. Best is trial 3 with value: 0.713969804788922.\n",
      "[I 2025-06-17 15:23:19,374] Trial 9 finished with value: 0.713565960484366 and parameters: {'kernel': 'sigmoid', 'gamma': 0.00714538199840599}. Best is trial 3 with value: 0.713969804788922.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.713969804788922\n",
      "Hyperparameters:  {'kernel': 'rbf', 'gamma': 0.0020622454773098968}\n",
      "Accuracy in test:  0.7128113080547454\n"
     ]
    }
   ],
   "source": [
    "# Execute the tuning and save the models as tuples\n",
    "\n",
    "models = {\n",
    "    'LightGBM': (lgbm_objective, LGBMClassifier),\n",
    "    'XGBoost': (xgb_objective, XGBClassifier),\n",
    "    'Stacking': (stacking_objective, StackingClassifier),\n",
    "    'Random Forest': (rf_objective, RandomForestClassifier),\n",
    "    'KNN': (knn_objective, KNeighborsClassifier),\n",
    "    'Decision Tree': (dt_objective, DecisionTreeClassifier),\n",
    "    'MLP': (mlp_objective, MLPClassifier),\n",
    "    'Gradient Boosting': (gbc_objective, GradientBoostingClassifier),\n",
    "    'SVM': (svm_objective, SVC),\n",
    "}\n",
    "\n",
    "\n",
    "for name, (objective, constructor) in models.items():\n",
    "\n",
    "    print(\"Optimizating\", name, \" ...\")\n",
    "    print(\"------------------------------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    study = optuna.create_study(direction = 'maximize')\n",
    "    study.optimize(objective, n_trials = 10)\n",
    "\n",
    "    print(\"Best accuracy: \", study.best_value)\n",
    "    print(\"Hyperparameters: \", study.best_params)\n",
    "\n",
    "    \n",
    "    # Train the model with the best hyperparameter fit\n",
    "\n",
    "    if name == 'MLP':\n",
    "        params = study.best_params.copy()\n",
    "        hl1 = params.pop('hl1')\n",
    "        hl2 = params.pop('hl2')\n",
    "        final_model = constructor(\n",
    "            hidden_layer_sizes = (hl1, hl2),\n",
    "            **params\n",
    "        )\n",
    "\n",
    "    elif name == 'Stacking':\n",
    "        params = study.best_params.copy()\n",
    "        passthrough = params.pop('passthrough')\n",
    "        C = params.pop('C')\n",
    "\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators = 50, random_state = 42)),\n",
    "            ('dt', DecisionTreeClassifier(min_samples_leaf = 2, random_state = 42)),\n",
    "            ('gbc', GradientBoostingClassifier(n_estimators = 50, random_state = 42))\n",
    "        ]\n",
    "\n",
    "        final_model = StackingClassifier(\n",
    "            estimators = estimators,\n",
    "            final_estimator = LogisticRegression(C = C, max_iter = 1000, random_state = 42),\n",
    "            passthrough = passthrough, n_jobs = -1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        final_model = constructor(**study.best_params)\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    acc_metric = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy in test: \", acc_metric)\n",
    "\n",
    "    # Save results\n",
    "\n",
    "    model_results[name] = {\n",
    "        'accuracy_test': acc_metric,\n",
    "        'model': final_model,\n",
    "        'parameters': study.best_params,\n",
    "        'study': study\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b10a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de precisin en test para cada modelo:\n",
      "\n",
      "LightGBM: Accuracy = 0.7221\n",
      "XGBoost: Accuracy = 0.7208\n",
      "Stacking: Accuracy = 0.7197\n",
      "Random Forest: Accuracy = 0.7186\n",
      "KNN: Accuracy = 0.6538\n",
      "Decision Tree: Accuracy = 0.7199\n",
      "MLP: Accuracy = 0.7152\n",
      "Gradient Boosting: Accuracy = 0.7199\n",
      "SVM: Accuracy = 0.7128\n"
     ]
    }
   ],
   "source": [
    "# Find the best parameters for each model\n",
    "\n",
    "print(\"\\nResumen de precisin en test para cada modelo:\\n\")\n",
    "\n",
    "for name, result in model_results.items():\n",
    "    accuracy = round(result['accuracy_test'], 4)\n",
    "    print(name + \": Accuracy = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
